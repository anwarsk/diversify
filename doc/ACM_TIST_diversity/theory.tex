\subsection{Optimizing Expected 1-call@k}
Before we present the result on optimizing expected n-call@k, we start off from the simplest case, expected 1-call@k (where $n=1$):

% Definition of 1-call
\begin{align}
    \ExpOneCall(S_k,\vec{q}) & = \mathbb{E}[R_k\geq 1|s_1,\dots,s_k,\vec{q}]
\end{align}

Since $R_k\geq 1$ is satisfied as long as any one of the $r_i$ is $1$, the objective can be rewritten in term of set notation:

\begin{align}
\label{eq:setRelevance}
    \ExpOneCall(S_k,\vec{q}) & = \mathbb{E} \left[\left. \bigvee_{i=1}^{k}r_i=1 \right| s_{1},\dots, s_{k},\vec{q} \right]
\end{align}

Since jointly optimizing $\ExpOneCall(S_k,\vec{q})$ is NP-hard, we
take a greedy approach similar to MMR where we choose the best $s_k^*$
assuming that $S_{k-1}^*$ is given.  Then following~\cite{chen06Less},
we can greedily optimize this objective as 
follows:

\begin{align}
s_k^* & = \argmax_{s_k} \; \ExpOneCall(S_{k-1}^* \cup \{ s_k \},\vec{q}) \nonumber \\
   & = \argmax_{s_k} \mathbb{E}\left[\left. \bigvee_{i=1}^{k} r_i=1 \right| S_{k-1}^*, s_{k},\vec{q}\right] \nonumber \\
\end{align}

We then applied a logical equivalence and exploited the additivity of
mutually exclusive events to split $\bigvee_{i=1}^{k} r_i=1$ into mutually exclusive
disjoint subsets:

\begin{align}
s_k^* & = \argmax_{s_k} \mathbb{E}\Bigg[  (r_1=1) \vee (r_2 =1 \wedge r_1=0) \vee (r_3 =1 \wedge r_2=0 \wedge r_1=0) \vee \cdots \vee \nonumber \\
   & \hspace{19mm} \left(r_k=1 \wedge \bigwedge_{i=1}^{k-1} r_i=0 \right) \, \Bigg| \, S_{k-1}^*,s_k,\vec{q} \Bigg] \label{eq:logicalsplit}
\end{align}

Since these events are binary and disjoint, we can rewrite the expectation as probability.
This gives us the sum of the probabilities of each individual event.
We further simplify it by grouping all $r_j = 0$.\footnote{The notation 
$\{ \cdot \}_C$ refers to a (possibly empty) set of 
variables (or variable assignments) $\cdot$ that meet constraints $C$.}
We factorize each joint probability into a conditional and prior, and removed terms and factors that are do not contain $s_k$, note that these terms are only acting as constants when we optimize for $s_k$:

\begin{align}   
s_k^* & = \argmax_{s_k} \sum_{i=1}^{k} P(  r_i=1, \{ r_{j}=0 \}_{j<i} \, | \, \{ s^*_j \}_{j\leq i,j<k},\{ s_{k} \}_{k=i},\vec{q}) \nonumber \\
   &= \argmax_{s_k} \sum_{i=1}^{k} P(  r_i=1 \, | \, \{ r_{j}=0 \}_{j<i} , \{ s^*_j \}_{j\leq i,j<k},\{ s_k \}_{k=i},\vec{q}) \nonumber\\
   & \hspace{21.5mm} P(\{ r_{j}=0 \}_{j<i} \, | \, \{ s^*_j \}_{j\leq i,j<k}, \vec{q}) \nonumber \\
   &= \argmax_{s_k} P( r_k=1 \, | \, \{ r_{j}=0 \}_{j<k}, S_{k-1}^*,s_k,\vec{q}) \label{eq:set-objective}
\end{align}

From~\eqref{eq:set-objective}, to optimize for $s_k$, we need only to maximize $s_k$'s probability of relevance conditioned on the previous selections (which are assumed irrelevant, $r_j=0$) and the query.

Next we evaluate the final query from~\eqref{eq:set-objective} w.r.t.\
our graphical model of subtopic relevance from Figure~\ref{fig:gm}:

\begin{align}
s_k^* & = \argmax_{s_{k}} P( r_k=1 \, | \, \{ r_{j}=0 \}_{j<k}, S_{k-1}^*,s_k,\vec{q}) \nonumber \\
& = \argmax_{s_{k}} \sum_{t, t_1, \cdots, t_{k}} P(t|\vec{q}) P(t_k|s_k) \I[t_k = t] \prod_{i=1}^{k-1} P(t_i|s_i^*) \I[t_i\neq t]\nonumber \\
& = \argmax_{s_{k}} \sum_{t} P(t|\vec{q}) \sum_{t_{k}} P(t_k|s_k) \I[t_k = t] \prod_{i=1}^{k-1} \sum_{t_{i}} P(t_i|s_i^*) \I[t_i \neq t]\nonumber \\
& = \argmax_{s_{k}} \sum_{t} P(t|\vec{q}) P(t_k=t|s_k) \fbox{$\prod\limits_{i=1}^{k-1} (1 - P(t_i=t|s_i^*))$} \label{eq:partial_simp}
\end{align}

Here we have used the following equality:

\begin{align*}
& \sum_{t_{i}} P(t_i|s_i) \I[t_i = t] =  P(t_i=t|s_i) \\
& \sum_{t_{i}} P(t_i|s_i) \I[t_i \neq t] = 1 - P(t_i=t|s_i)
\end{align*}

Defining $\tilde{P}(t | S_{k-1}^*) = 1 - \Box = 1 - \prod_{i=1}^{k-1} (1 -
  P(t_i=t|s_i^*))$, this is the probability that 
set $S_{k-1}^*$ already \emph{covers} topic $t$ 
w.r.t.\ a \emph{noisy-or} interpretation.  Substituting
$(1 - \tilde{P}(t | S_{k-1}^*))$ for $\Box$ since 
$(1 - \tilde{P}(t | S_{k-1}^*)) = 1 - (1 - \Box) = \Box$, we obtain

\begin{align}
s_k^* & = \argmax_{s_{k}} \sum_{t} P(t|\vec{q}) P(t_k=t|s_k) \left( 1 - \tilde{P}(t | S_{k-1}^*) \right) \nonumber \\
      & = \argmax_{s_{k}} \sum_{t} \underbrace{P(t|\vec{q}) P(t_k=t|s_k)}_{\mbox{\footnotesize query similarity}} - \sum_{t} \underbrace{P(t|\vec{q}) P(t_k=t|s_k) \tilde{P}(t | S_{k-1}^*)}_{\mbox{\footnotesize query-reweighted diversity}}. \label{eq:1call}
\end{align}

From~\eqref{eq:1call}, we can see that optimizing expected 1-call@k give us a greedy algorithm that resemblances MMR with $\lambda = 1/2$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Optimizing Expected n-call@k}
Note that it is not straight-forward to use the set-based representation of the expected n-call@k objective for a larger $n$: even for $n=2$, the number of disjoint events that satisfy $R_k \geq 2$ grows by a factor of $(k-1)/2$.\footnote{$\binom{k}{2} / \binom{k}{1} = (k-1)/2$}
The derivation of optimizing expected 2-call@k (using set notation) is presented in Appendix~\ref{appendix.2call}, which aims to provide an intuitive support to the derivation of optimizing expected n-call@k. 
In the following derivation, we adopt a more abstract approach (working with $R_k$ directly) while utilizing the same principle as in expected 1-call@k.
% We cast the optimization of $\ExpNCall{n}(S_k,\vec{q})$
% in a form similar to MMR in~\eqref{eq:MMR} to 
%determine the correspondence between $\lambda$ and the
%result of this derivation. 
As mentioned, a greedy approach selects $s_k$ assuming that $S_{k-1}^*$ is already chosen:

\begin{align}
  s_k^* & = \argmax_{s_k} \mathbb{E}[R_k\geq n|S_{k-1}^*,s_k,\vec{q}] \nonumber\\[-1mm]
  & = \argmax_{s_k} P(R_k\geq n|S_{k-1}^*,s_k,\vec{q}) \nonumber 
\end{align}

Here, we have exploited the binary ($0,1$) nature of $R_k \geq n$ to rewrite the objective
directly as a probabilistic query.
This query can be evaluated w.r.t.\ our latent subtopic binary relevance
model in Figure~\ref{fig:gm} as follows, where we marginalize out
all non-query, non-evidence variables $T_k$ $\big($define
$T_k\!=\!\{t,t_1,\dots,t_k\}$ and 
$\sum_{T_k} \circ = \sum_t \sum_{t_1} \cdots \sum_{t_k} \circ \big)$:

\begin{align}
  s_k^* = & \argmax_{s_k} \!\sum_{T_k} \Bigl( P(t|\vec{q}) \,P(t_k|s_k) \prod_{i=1}^{k-1} P(t_i|s_i^*) \cdot P(R_k\geq n|T_k,S_{k-1}^*,s_k,\vec{q}) \Bigr) \nonumber 
\end{align}

We split $R_k \geq n$ into two disjoint (additive) events
$(r_k \! \geq \! 0, R_{k\!-\!1}\!\geq \!n)$, $(r_k\!\!=\!\!1, R_{k\!-\!1}\!\!=\!\!n\!-\!1)$ based on $R_{k-1}$. (If $R_{k-1}$ is equal to $n\!-\!1$, $r_k$ must be $1$; if $R_{k-1}$ is greater or equal to $n$, then $r_k$ can be either $0$ or $1$). 

\begin{align}
 s_k^* = & \argmax_{s_k} \!\sum_{T_k} P(t|\vec{q}) \,P(t_k|s_k) \prod_{i=1}^{k-1} P(t_i|s_i^*) \cdot \Bigl( P(r_k\geq 0, R_{k-1}\geq n|T_k,S_{k-1}^*,s_k,\vec{q}) \nonumber \\
  & \hspace{56mm} + P(r_k = 1, R_{k-1} = n-1|T_k,S_{k-1}^*,s_k,\vec{q}) \Big) \nonumber 
\end{align}

We then write the joint probability into a conditioned and prior, conditioned on $R_k$:
% Also note that all $r_i$ are D-separated:

\begin{align}
 s_k^* = & \argmax_{s_k} \!\sum_{T_k} P(t|\vec{q}) \,P(t_k|s_k) \prod_{i=1}^{k-1} P(t_i|s_i^*) \cdot \Bigl( \mbox{$\underbrace{P(r_k\!\geq\!0|R_{k-\!1}\!\geq\!n,t_k,t)}_{1}$} P(\!R_{k-\!1}\!\geq\!n|\TlessK) \nonumber \\
  & \hspace{50mm} + P(r_k=1|R_{k-1}\!=\!n\!-\!1,t_k,t) P(\!R_{k-\!1}\!=\!n\!-\!1|\TlessK) \Big) \nonumber 
\end{align}

We distribute initial terms over the summands noting that 
$\sum_{t_k} \!\! P(t_k|s_k) P(r_k\!\!=\!\!1|t_k,t) \! = \!\! \sum_{t_k} \!\! P(t_k|s_k) \I[t_k\!\!=\!\!t] \! = \!\! P(t_k\!\!=\!\!t|s_k)$,
and $r_k$ is independent to $R_k$ given $T_k$:

\begin{align}
 s_k^* = & \argmax_{s_k} \Bigg( \sum_{\TlessK} \bigg[ \mbox{$\underbrace{ \sum_{t_k} \!P(t_k|s_k) }_{1}$} \bigg] P(\!R_{\!k-\!1}\!\geq\!n|\TlessK) P(t|\vec{q}) \prod_{i=1}^{k-1} P(t_i|s_i^*) \nonumber \\[-3.5mm]
  & \hspace{13mm} + \sum_{t} \!P(t|\vec{q}) P(t_k\!=\!t|s_k) \hspace{-4mm} \sum_{t_1, \dots, t_{k-1}} \hspace{-3mm} P(R_{k-\!1}\!=\!n\!-\!1|\TlessK) \prod_{i=1}^{k-1} \!P(t_i|s_i^*) \Bigg) \nonumber
\end{align}

Next we proceed to drop the first summand since it is not a function of $s_k$ (\emph{i.e.},
it has no influence in determining $s_k^*$):
% This give us the
%simplified optimization objective:

\begin{align}
s_k^* = & \argmax_{s_k} \!\sum_{t} \!P(t|\vec{q}) P(t_k\!=\!t|s_k) P(\!R_{k-\!1}\!\!=\!n\!-\!1|S_{k-1}^*, t) \label{eq.ncall}
\end{align}

By similar reasoning, we can derive that the last probability 
needed in~\eqref{eq.ncall} is recursively defined as 
% $P(R_k=n|S_k,t)=$

\begin{align*}
P(R_k=n|S_k,t)=
\begin{cases}
n \geq 1, k > 1:  &  \bigl( 1\!-\!P(t_k\!=\!t|s_k) \bigr) P(R_{k-1}\!=\!n|S_{k-1},t) \nonumber \\
  & \hspace{5mm} + P(t_k\!=\!t|s_k) P(R_{k-\!1}\!=\!n\!-\!1|S_{k-\!1},t) \\
%%
n = 0, k > 1:   & \bigl( 1\!-\!P(t_k\!=\!t|s_k) \bigr) P(R_{k-\!1}\!=\!0|S_{k-\!1},t) \\
%%
n = 1, k = 1:   & P(t_1\!=\!t|s_1) \\
n = 0, k = 1:   & 1 - P(t_1\!=\!t|s_1) \\
n > k:			& 0
\end{cases}
%       \bigl 1-P(t_1\!=\!t|s_1) \bigr) = \bigl 1-P(t_1\!=\!t|s_1) \bigr) \\
%  & \hspace{2mm} P(R_1\!=\!1|S_1,t) = P(t_1\!=\!t|s_1)
\end{align*}

We can now rewrite~\eqref{eq.ncall} by unrolling its recursive definition.
For expected n-call@k where $n \leq k/2$ %, n \! \neq \! 1$ 
(a symmetrical result holds for $k/2 < n \leq k$)\footnote{Refer to Appendix~\ref{appendix.ncall} for details.}, the explicit unrolled objective is 

\begin{align}
  & s_k^* = \argmax_{s_k} \sum_t \Biggl( P(t|\vec{q}) \, P(t_k=t|s_k) \cdot \hspace{-2mm} \sum_{j_1, \dots, j_{n-\!1}} \hspace{-14mm} \prod_{\hspace{14.5mm} l \in \{j_1, \dots, j_{n-\!1}\}} \hspace{-14mm} P(t_l\!=\!t|s_l^*) \hspace{-13mm} \prod_{\substack{i=1 \\ \hspace{14mm} i \notin \{j_1, \dots, j_{n-\!1}\}}}^{k-1} \hspace{-13mm} \!\bigl( 1 - P(t_i\!=\!t|s_i^*) \bigr) \!\Biggr) \label{eq.ncall.alt}
\end{align}
where $j_1, \dots, j_{n-1} \in \{1,\ldots,k-1\}$ satisfy 
that $j_i < j_{i+1}$ (\emph{i.e.},
an ordered permutation of $n-1$ result set indices).
Note that~\eqref{eq.ncall.alt} reduces to~\eqref{eq:partial_simp} when $n=1$.

%A similar objective can be easily obtained for the case $n>k/2$,
%$n\neq k$ \emph{via} the same process (not shown due to space).

% ===============================================================================

%From here we focus on the last product in~\eqref{eq.ncall.alt}.  We note that
If we assume each document covers a single subtopic of the query (\emph{e.g.},
a subtopic represents an intent of an ambiguous query) then we can assume that 
$\forall i \; P(t_i|s_i) \in \{0,1\}$ and $P(t|\vec{q}) \in \{0,1\}$.  This
allows us to convert a $\prod$ to a $\max$ 
\begin{align*}
  \hspace{-13mm} \prod_{\substack{i=1 \\ \hspace{14mm} i \notin \{j_1, \dots, j_{n-\!1}\}}}^{k-1} \hspace{-14mm} \bigl( 1 - \!P(t_i\!=\!t|s_i^*) \bigr) & =
1 - \Biggl( 1 - \hspace{-14mm} \prod_{\substack{i=1 \\ \hspace{14mm} i \notin \{j_1, \dots, j_{n-\!1}\}}}^{k-1} \hspace{-13mm} \bigl( 1 - P(t_i\!=\!t|s_i^*) \bigr) \Biggr) 
   = 1 - \Bigl( \hspace{-6mm} \max_{\substack{i \in [1,k-1] \\ \hspace{7mm} i \notin \{j_1, \dots, j_{n-1}\}}} \hspace{-6mm} P(t_i\!=\!t|s_i^*) \Bigr)
\end{align*}
and by substituting this into~\eqref{eq.ncall.alt} and distributing, we get
\begin{align}
s_k^* =  & \, \argmax_{s_k} \sum_t \Biggl( P(t|\vec{q}) P(t_k\!=\!t|s_k)  \sum_{\hspace{-1mm} j_1, \dots, j_{n-\!1}} \hspace{-13.5mm} \prod_{\hspace{14.5mm} l \in \{j_1, \dots, j_{n-1}\}} \hspace{-14mm} P(t_l\!=\!t|s_l^*) \nonumber \\[-2mm]
  & \hspace{20mm} - \!P(t|\vec{q}) P(t_k\!=\!t|s_k) \sum_{\hspace{-1mm} j_1, \dots, j_{n-\!1}} \hspace{-14mm} \prod_{\hspace{14.5mm} l \in \{j_1, \dots, j_{n-\!1}\}} \hspace{-13mm} P(t_l\!=\!t|s_l^*) \hspace{-11.5mm} \max_{\substack{\hspace{5mm} i \in [1,k-1] \\ \hspace{11.5mm} i \notin \{j_1, \dots, j_{n-\!1}\}}} \hspace{-8.5mm} P(t_i\!=\!t|s_i^*) \!\!\Biggr) . \nonumber %\\[-2mm]
%  & \hspace{3mm} - \!P(t|\vec{q}) P(t_k\!=\!t|s_k) \hspace{-1mm} \mbox{$\underbrace{\sum_{\hspace{-5mm} j_1, \dots, j_{n-\!1}} \hspace{-14mm} \prod_{\hspace{14.5mm} l=\{j_1, \dots, j_{n-\!1}\}} \hspace{-15mm} P(t_l\!=\!t|s_l^*) \hspace{-11.5mm} \max_{\substack{\hspace{5mm} i=[1,k-1] \\ \hspace{11.5mm} i \notin \{j_1, \dots, j_{n-\!1}\}}} \hspace{-9.5mm} P(t_i\!=\!t|s_i^*)}_{}$} \!\!\Biggr) \nonumber \\[-3.5mm]
%  & \hspace{40mm} \max_{s_i \in S_{k-1}^*} P(t_i\!=\!t|s_i) \, w_i \nonumber\\
\end{align}
Assuming $m$ of the selected documents $(S_{k-1}^*)$ are relevant 
then the top term
(specifically $\prod_l$) is non-zero $\binom{m}{n-1}$ times.  For the
bottom term, it takes $n-1$ relevant $S_{k-1}^*$ to satisfy its
$\prod_l$, and one additional relevant document to satisfy the
$\max_i$ making it non-zero $\binom{m}{n}$ times.  Factoring out the
$\max$ element from the bottom and pushing the $\sum_t$ inwards (all legal
due to the $\{0,1\}$ subtopic probability assumption) we get
\begin{align}
s_k^* = & \argmax_{s_k} \binom{m}{n\!-\!1} \underbrace{\sum_t P(t|\vec{q}) P(t_k\!=\!t|s_k)}_{\textrm{relevance}: \; \Sim_1(s_k,\vec{q})} - \binom{m}{n} \max_{s_i \in S_{k-1}^*} \underbrace{\sum_t \!P(t|\vec{q}) P(t_k\!=\!t|s_k) P(t_i\!=\!t|s_i)}_{\textrm{diversity}: \; \Sim_2(s_k,s_i,\vec{q})} \nonumber .\\[-7mm] \nonumber
%=  & \argmax_{s_k} \sum_t \Bigg[ \binom{m}{n-1} P(t|\vec{q}) P(t_k\!=\!t|s_k) \nonumber \\[-2mm]
%  & \hspace{17.5mm} - \binom{m}{n} \max_{s_i \in S_{k-1}^*} P(t_i\!=\!t|s_i) \!P(t|\vec{q}) P(t_k\!=\!t|s_k) \Bigg] \nonumber
\end{align}  
From here we can 
normalize by $\binom{m}{n-1} + \binom{m}{n} = \binom{m+1}{n}$ 
(Pascal's rule), leading to fortuitous cancellations and the result:
\begin{align}
s_k^* =  & \argmax_{s_k} \!\! \frac{n}{m\!+\!1} \Sim_1(s_k,\vec{q}) - \frac{m\!-\!n\!+\!1}{m+1} \max_{s_i \in S_{k-1}^*} \! \Sim_2(s_k,s_i,\vec{q}) \label{eq:ncall-result} \\ \nonumber
\end{align}
Fortuitously, we note that the $\binom{m+1}{n}$ divisor cancelled with
the numerators, yielding this elegant and interpretable result.
Comparing to MMR in~\eqref{eq:MMR}, we can clearly see that $\lambda =
\frac{n}{m\!+\!1}$.  Assuming $m = n$
%$m \approx n$
since expected n-call@k optimizes for the case where $n$ relevant documents are selected, then $\lambda = \frac{n}{n\!+\!1}$, which achieves our goal of formally expressing the relevance vs. diversity tradeoff as a function of $n$, $k$. In practice, under the greedy approach of the expected n-call@k in selecting $S_k^*$, we expect that there are already $n$ relevant documents chosen in the set $S_{k-1}^* = \{s_1^*, \dots, s_{k-1}^*\}$, and hence in expectation $m=n$.
%, and $m$.
Overall we have achieved our goal and have shown that
indeed, diversificiation in expected $n$-call@$k$ decreases linearly 
as $n \to 1$.

%As a reality check, we see that this coincides
%with the published result of $\lambda=0.5$ in~\cite{sanner11} for
%$n=1$, $m=1$.  Overall we have achieved our goal and have shown that
%indeed, diversificiation in expected $n$-call@$k$ decreases linearly 
%as $n \to 1$.

% ===============================================================================
% This subsection is copied from Section 4.1
\subsection{Discussion}

The result in~\eqref{eq:ncall-result} is strikingly similar to MMR --- it
contains two terms, one for query similarity and the other for result
set diversification, where each term represents a similarity kernel
--- more specifically a \emph{probability product kernel}
(PPK)~\cite{prodprobkernel} that is an inner product of probability
vectors (or more generally, functions).  More formally, let
$\vec{T}'$, $\vec{T}_k$, and $\vec{T}_{S_{k-1}^*}$ be respective topic
probability vectors $P(t'=t|\vec{q})$, $P(t_k=t|s_k)$ and
%$\tilde{P}(t_k=t | S_{k-1}^*)$ 
$P(t_i\!=\!t|s_i)$
with vector indices for each topic $t
\in T$.  Then the similarity and diversity terms from~\eqref{eq:ncall-result}
can be respectively written as
\begin{align}
%\Sim_1(\vec{q},s_k) = & \hspace{-3mm}
\sum_{t \in T} P(t'=t|\vec{q}) P(t_k=t|s_{k}) & \; = \; \langle \vec{T}',\vec{T}_k \rangle \label{eq:sim_term} \; \mbox{and}\\
%\Sim_2(s_k,S_{k-1}) = & \hspace{-3mm}
\sum_{t \in T} P(t|\vec{q}) P(t_k=t|s_k) \tilde{P}(t | S_{k-1}^*) & \; = \; \langle \vec{T}_k, \vec{T}_{S_{k-1}^*} \rangle_{\vec{T}'}. \label{eq:div_term}
\end{align}
Here, we let $\langle \cdot,\cdot \rangle$ denote an inner product of
two vectors and $\langle \cdot,\cdot \rangle_\vec{v}$ a
\emph{$\vec{v}$-reweighted} inner product, defined as
in~\eqref{eq:div_term}.

While having similarity and diversity terms similar to MMR,
Exp-$n$-call@$k$ in~\eqref{eq:ncall-result} clearly differs from MMR:
\begin{enumerate}
\item While MMR's definition allows for any similarity function, not
just PPKs, we note that \emph{equating words to subtopics}, popular
kernels like TF and TFIDF~\cite{salton83Introduction} can be viewed
directly as PPKs if the TF and TFIDF vectors are $L_1$ normalized to
represent probability vectors.
\item MMR uses a maximization term for
diversity, whereas optimization of Exp-$n$-call@$k$ instead calls for
a product (noisy-or) diversity term. %$\tilde{P}(t | S_{k-1}^*)$.
We note that a noisy-or reduces to a max when the subtopic
probabilities are deterministic (0 or 1).
\item While MMR proposes a $\lambda$ term to explicitly
trade off the similarity and diversity terms, the greedy optimization
of Exp-$n$-call@$k$ in~\eqref{eq:ncall-result} yields such trade-off term
as a function of $m, n$ and $k$.
%(or alternately, an implicit $\lambda=.5$). Although it seems a tunable
%$\lambda$ is not needed for maximizing Exp-$1$-call@$k$, it may be
%desirable when maximizing surrogate retrieval objectives (e.g., ranking
%objectives).
\item Optimizing Exp-$n$-call@$k$ introduces query-specific relevance into
the diversification term as shown
by the query topic ($\vec{T}'$) reweighted
diversity function in~\eqref{eq:div_term}.
\end{enumerate}

%To verify whether the differences between MMR and Exp-$1$-call@$k$
%matter empirically, we compare the two algorithms across a number of
%metrics on three diversity testbeds: the TREC 6-8 Interactive
%Track\footnotemark[1] (17 queries) and 2009 and 2010 ClueWeb Diversity
%tasks of the TREC Web Track\footnotemark[2] (50 queries each).  On
%these testbeds, we evaluate \emph{mean subtopic
%recall@$k$}~\cite{zhai03Beyond} (fraction of total annotated
%aspects/subtopics covered by a result set at rank $k$, averaged over
%queries), which is an appropriate loss function for the
%\emph{set-level} metric~\eqref{eq:setRelevance}~\cite{chen06Less}.  We
%also evaluate a variety of more recent \emph{rank-based} diversity evaluation
%metrics such as intent-aware expected reciprocal rank
%(ERR-IA@$k$)~\cite{err-ia}, $\alpha$-nDCG@$k$~\cite{clarke08Novelty},
%and intent-aware mean average precision
%(MAP-IA)~\cite{agrawal09diversifying}.
%
%We use MMR with $\lambda = 0.5$ to match the equal weighting of
%similarity and diversity in Exp-$1$-call@$k$.  An
%LDA~\cite{blei03Latent} topic model is trained on the top-100 OKAPI
%BM25~\cite{bm25} results for each query (on its respective collection)
%and these subtopic distributions are used for the similarity and
%diversity kernels in both algorithms: for MMR we choose $\Sim_1$ and
%$\Sim_2$ kernels as in~\eqref{eq:sim_term} --- effectively LDA
%variants of latent semantic indexing (LSI)~\cite{deerwester90LSA}
%kernels; for Exp-$1$-call@$k$, we use the similarity and diversity
%kernels respectively defined in~\eqref{eq:sim_term}
%and~\eqref{eq:div_term}.  Both MMR and Exp-$1$-call@$k$ are used to
%rank the top-20 documents from the top-100 OKAPI BM25 results.
%
%Results in Table~\ref{table:different_metrics} and
%Figure~\ref{fig:mmr_vs_1call} show the performances of MMR and
%Exp-$1$-call@$k$ on the three diversity testbeds across various
%diversity measures; although there are minor performance differences,
%we note that these differences are not statistically significant
%w.r.t.\ 95\% confidence intervals.  Nonetheless, the results appear to
%indicate that the structural similarities in the use of MMR and the
%optimization of Exp-$1$-call@$k$ outweigh the differences in this
%evaluation.